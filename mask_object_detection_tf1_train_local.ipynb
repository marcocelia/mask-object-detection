{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mask-object-detection-tf1-train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "19HrNfhqkH7C"
      ]
    },
    "kernelspec": {
      "name": "python379jvsc74a57bd0d3f4e3eb3c6016b5dbc42a6cf944ff78b7498167359f3c1635e6b48b7883057a",
      "display_name": "Python 3.7.9 64-bit"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6Tti67UjwW4"
      },
      "source": [
        "# **Mask Object Detection - Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGAg_0b-j-U4"
      },
      "source": [
        "## **Environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqtT31nPn2bH"
      },
      "source": [
        "### **Configurations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr7SxvuHn1vq"
      },
      "source": [
        "import os\n",
        "\n",
        "# Repo URL\n",
        "repo_url = 'https://github.com/marcocelia/mask-object-detection'\n",
        "\n",
        "# Models\n",
        "MODELS_CONFIG = {\n",
        "    'ssd_inception_v2_coco': {\n",
        "        'model_name': 'ssd_inception_v2_coco_2018_01_28',\n",
        "        'model_path': '/models/tf1/my_ssd_inception_v2_coco/',\n",
        "        'pipeline_file': 'pipeline.config'\n",
        "    },\n",
        "    'faster_rcnn_inception_v2_coco': {\n",
        "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
        "        'model_path': '/models/tf1/my_faster_rcnn_inception_v2_coco/',\n",
        "        'pipeline_file': 'pipeline.config'        \n",
        "    }\n",
        "}\n",
        "\n",
        "# Select a model to use.\n",
        "selected_model = 'faster_rcnn_inception_v2_coco'\n",
        "\n",
        "model_name = MODELS_CONFIG[selected_model]['model_name']\n",
        "model_path = MODELS_CONFIG[selected_model]['model_path']\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "# Set Repository Home Directory\n",
        "base_dir = '/home/marcocelia/personal/tesi'\n",
        "repo_dir_path = os.path.abspath(os.path.join(base_dir, os.path.basename(repo_url)))\n",
        "\n",
        "# Set Label Map (.pbtxt) path and pipeline.config path\n",
        "label_map_pbtxt_fname = repo_dir_path + '/annotations/label_map.pbtxt'\n",
        "pipeline_fname = repo_dir_path + model_path + pipeline_file\n",
        "\n",
        "# Set .record path\n",
        "test_record_fname = repo_dir_path + '/annotations/test.record'\n",
        "train_record_fname = repo_dir_path + '/annotations/train.record'\n",
        "\n",
        "# Set output directories and clean up\n",
        "model_dir = repo_dir_path + '/training/'\n",
        "output_dir = repo_dir_path + '/exported-models/'\n",
        "\n",
        "print(f'repo_dir_path: {repo_dir_path}')\n",
        "print(f'model_dir: {model_dir}')\n",
        "print(f'pipeline_fname: {pipeline_fname}')\n",
        "print(f'output_dir: {output_dir}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### **Clean previous execution**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf {base_dir}/cocoapi \n",
        "!rm -rf {base_dir}/models \n",
        "!rm -rf {repo_dir_path}/images/test\n",
        "!rm -rf {repo_dir_path}/images/train\n",
        "!rm -rf {repo_dir_path}/exported-models\n",
        "!rm -rf {repo_dir_path}/training\n",
        "!rm -rf {repo_dir_path}/annotations/*.csv\n",
        "!rm -rf {repo_dir_path}/annotations/*.record\n",
        "!rm -rf {repo_dir_path}/{selected_model}_train.tar.gz\n",
        "!rm -rf {repo_dir_path}/{selected_model}_finetuned.tar.gz"
      ]
    },
    {
      "source": [
        "### **Clone TF model repo**"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "n3yLH5NTvhcP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpJJ5-VDvota"
      },
      "source": [
        "%cd {base_dir}\n",
        "!git clone -b r1.13.0 https://github.com/tensorflow/models.git\n",
        "\n",
        "%cd {base_dir}/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += f':{base_dir}/models/research/:{base_dir}/models/research/slim/'\n",
        "\n",
        "!pip install .\n",
        "\n",
        "# Test\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n82Ffqi7o-m8"
      },
      "source": [
        "### **Install COCO evaluation metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnRFNQsipHZW"
      },
      "source": [
        "%cd {base_dir}\n",
        "!git clone --quiet https://github.com/cocodataset/cocoapi.git\n",
        "%cd {base_dir}/cocoapi/PythonAPI\n",
        "!make\n",
        "!cp -r pycocotools {base_dir}/models/research/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C4ZYfIrwdgo"
      },
      "source": [
        "### **Download pretrained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWShV93NwiLy"
      },
      "source": [
        "%cd {base_dir}/models/research\n",
        "%rm -rf pretrained_model/\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "MODEL_FILE = model_name + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = f'{base_dir}/models/research/pretrained_model/'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(model_name, DEST_DIR)\n",
        "\n",
        "# Check downloaded files\n",
        "!echo {DEST_DIR}\n",
        "!ls -alh {DEST_DIR}\n",
        "\n",
        "# Set fine tune checkpoint\n",
        "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
        "print(\"fine_tune_checkpoint: \", fine_tune_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### **Check project repository**"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "HiMK3dbO0E9i"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYCEU01E0LIy"
      },
      "source": [
        "import os\n",
        "%cd {base_dir}\n",
        "\n",
        "# # Check if label map and pipeline files exist\n",
        "assert os.path.isfile(label_map_pbtxt_fname), '`{}` not exist'.format(label_map_pbtxt_fname)\n",
        "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)\n",
        "\n",
        "!rm -rf {model_dir} {output_dir}\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSu4hUS-2oB-"
      },
      "source": [
        "## **Prepare dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4a0_QSd2wV0"
      },
      "source": [
        "### **Split train/test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuQUHC892sVE"
      },
      "source": [
        "%cd {repo_dir_path}\n",
        "\n",
        "# Split images to train:test = 7:3\n",
        "!python scripts/partition_dataset.py -x -i images/ -r 0.3\n",
        "\n",
        "# Check test images\n",
        "!ls images/test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mlY9-WR28Le"
      },
      "source": [
        "### **xml to csv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZqONePl3Ab6"
      },
      "source": [
        "# Create train data:\n",
        "!python scripts/xml_to_csv.py -i images/train -o annotations/train_labels.csv\n",
        "\n",
        "# Create test data:\n",
        "!python scripts/xml_to_csv.py -i images/test -o annotations/test_labels.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkfy5oq2Gs3s"
      },
      "source": [
        "### **Create TF record**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQs51LyyHBsH"
      },
      "source": [
        "!cat {label_map_pbtxt_fname}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T01mAZDEHHkV"
      },
      "source": [
        "# Create train data:\n",
        "!python {repo_dir_path}/scripts/generate_tfrecord_v1.py \\\n",
        "    --csv_input={repo_dir_path}/annotations/train_labels.csv \\\n",
        "    --output_path={repo_dir_path}/annotations/train.record \\\n",
        "    --img_path={repo_dir_path}/images/train \\\n",
        "    --label_map {repo_dir_path}/annotations/label_map.pbtxt\n",
        "\n",
        "# Create test data:\n",
        "!python {repo_dir_path}/scripts/generate_tfrecord_v1.py \\\n",
        "    --csv_input={repo_dir_path}/annotations/test_labels.csv \\\n",
        "    --output_path={repo_dir_path}/annotations/test.record \\\n",
        "    --img_path={repo_dir_path}/images/test \\\n",
        "    --label_map {repo_dir_path}/annotations/label_map.pbtxt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4b_nrgPHXO9"
      },
      "source": [
        "# Check\n",
        "assert os.path.isfile(test_record_fname), '`{}` not exist'.format(test_record_fname)\n",
        "assert os.path.isfile(train_record_fname), '`{}` not exist'.format(train_record_fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0m22aqHH5xU"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW4mgZfqZeq5"
      },
      "source": [
        "# train config\n",
        "!cat {pipeline_fname}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAnC1V7PJBIM"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT2h9avoIAYW"
      },
      "source": [
        "# Set log directory for tensorboard to watch\n",
        "LOG_DIR = model_dir\n",
        "\n",
        "# Clean up the directory\n",
        "!rm -rf {LOG_DIR}/*\n",
        "!ls -lrt {LOG_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdhdOPj-JLDs"
      },
      "source": [
        "%cd {repo_dir_path}\n",
        "!python {base_dir}/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jooG91EVSK6j"
      },
      "source": [
        "# Check the generated files\n",
        "!ls -lrt {model_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flXUWrE0SSq7"
      },
      "source": [
        "# Archive all the output\n",
        "%cd {repo_dir_path}\n",
        "!tar zcvf {selected_model}_train.tar.gz {model_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKq1XLznUDdR"
      },
      "source": [
        "## **Export**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxF7xAKjV2jW"
      },
      "source": [
        "### **Make trained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIbCPIu5UKsL"
      },
      "source": [
        "%cd {repo_dir_path}\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "lst = os.listdir(model_dir)\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "\n",
        "!python {base_dir}/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory={output_dir} \\\n",
        "    --trained_checkpoint_prefix={last_model_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyB-YRnhRdV5"
      },
      "source": [
        "# Check the output files\n",
        "!echo {output_dir}\n",
        "!ls -lsr {output_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0unN2ohHWNXd"
      },
      "source": [
        "# archive\n",
        "%cd {repo_dir_path}\n",
        "!tar zcvf {selected_model}_finetuned.tar.gz {output_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## **Evaluate performances**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "#  Change these values for the model used\n",
        "num_classes = 1\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "# Use images in test dir\n",
        "IMAGE_DIR = os.path.join(repo_dir_path, \"images\", \"test\")\n",
        "IMAGE_PATHS = []\n",
        "for file in os.listdir(IMAGE_DIR):\n",
        "    if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
        "        IMAGE_PATHS.append(os.path.join(IMAGE_DIR, file))\n",
        "\n",
        "# Set paths to the trained model\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "PATH_TO_CKPT = os.path.join(os.path.abspath(output_dir), \"frozen_inference_graph.pb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform the inference\n",
        "%cd {base_dir}/models/research/object_detection\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "# Set tensorflow graph\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "# Set categories\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, \n",
        "    max_num_classes=num_classes, \n",
        "    use_display_name=True\n",
        ")\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Convert input image to a numpy array\n",
        "def load_image_to_numpy(image):\n",
        "    (w, h) = image.size\n",
        "    return np.array(image.getdata()).reshape((h, w, 3)).astype(np.uint8)\n",
        "\n",
        "# Inference pipeline\n",
        "def run_inference(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in ['num_detections', 'detection_boxes', 'detection_scores','detection_classes', 'detection_masks']:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
        "\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
        "\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                \n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(detection_masks_reframed, 0)\n",
        "\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            start_time = time.time()\n",
        "            output_dict = sess.run(tensor_dict,feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "            end_time = time.time()\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['elapsed_ms'] = (end_time - start_time)*100\n",
        "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "\n",
        "    return output_dict\n",
        "\n",
        "i_times = []\n",
        "\n",
        "# Run the inference for each image\n",
        "for image_path in IMAGE_PATHS:\n",
        "    image = Image.open(image_path)\n",
        "    # Conver the image to numpy array\n",
        "    image_np = load_image_to_numpy(image)\n",
        "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Perform the interence\n",
        "    output_dict = run_inference(image_np, detection_graph)\n",
        "    \n",
        "    i_times.append(output_dict['elapsed_ms'])\n",
        "\n",
        "    # Visualize\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks'),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=8\n",
        "    )\n",
        "    plt.figure(figsize=IMAGE_SIZE)\n",
        "    plt.imshow(image_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"average inference time: {np.mean(i_times)} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}